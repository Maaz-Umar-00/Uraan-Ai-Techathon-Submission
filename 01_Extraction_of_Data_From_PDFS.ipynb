{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27ef6f8",
   "metadata": {},
   "source": [
    "## This file contains the code of extraction from Pdf's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "extract_tables_multi.py\n",
    "---------------------------------\n",
    "Robust PDF table extraction helper using pdfplumber + pandas.\n",
    "\n",
    "This script is designed to process multiple PDF files (for example, five PSDP reports),\n",
    "extract all tables found on each page, save each table as a CSV under\n",
    "`documents/<pdf_basename>/tables/` and write a small Markdown summary per PDF\n",
    "under `documents/<pdf_basename>/summary.md`.\n",
    "\n",
    "Key features:\n",
    "- Safe handling when a PDF has zero tables.\n",
    "- Attempts to use the first row of a table as header; falls back to numeric column names.\n",
    "- Saves example rows (head) in the Markdown summary for quick review.\n",
    "- Command-line friendly: pass a list of PDF paths or a glob pattern.\n",
    "\n",
    "Usage (from project root):\n",
    "    python documents/extract_tables_multi.py \"Expenditure-Summary-2025-26-Sept-2025.pdf\" \"other_report.pdf\"\n",
    "\n",
    "Or process many with a glob (PowerShell / bash):\n",
    "    python documents/extract_tables_multi.py \"docs/*.pdf\"\n",
    "\n",
    "Requirements:\n",
    "    pip install pdfplumber pandas\n",
    "\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path: Path):\n",
    "    \"\"\"Extract tables from a single PDF file using pdfplumber.\n",
    "\n",
    "    Returns a list of pandas.DataFrame objects and a brief report dict.\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    report = {\"pdf\": str(pdf_path.name), \"tables_found\": 0, \"table_shapes\": []}\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            try:\n",
    "                page_tables = page.extract_tables()\n",
    "            except Exception as e:\n",
    "                # If extraction fails for a page, continue but log the issue\n",
    "                print(f\"Warning: failed to extract tables from {pdf_path.name} page {page_num}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for i, raw_table in enumerate(page_tables, start=1):\n",
    "                if not raw_table:\n",
    "                    continue\n",
    "\n",
    "                # Coerce to DataFrame. Prefer the first row as header when it looks like strings\n",
    "                header = raw_table[0]\n",
    "                body = raw_table[1:]\n",
    "\n",
    "                # Clean header: if header row contains many None or numeric-like entries, fallback\n",
    "                header_valid = any(h and isinstance(h, str) and h.strip() for h in header)\n",
    "\n",
    "                if header_valid:\n",
    "                    df = pd.DataFrame(body, columns=[str(h).strip() for h in header])\n",
    "                else:\n",
    "                    # Generate numeric column names\n",
    "                    max_cols = max(len(r) for r in raw_table)\n",
    "                    cols = [f\"col_{i+1}\" for i in range(max_cols)]\n",
    "                    # Pad rows to equal length\n",
    "                    padded = [list(r) + [None] * (max_cols - len(r)) for r in raw_table]\n",
    "                    df = pd.DataFrame(padded[1:], columns=cols)\n",
    "\n",
    "                # Basic cleanup: strip whitespace from string columns\n",
    "                for c in df.columns:\n",
    "                    if df[c].dtype == object:\n",
    "                        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "                tables.append(df)\n",
    "                report[\"tables_found\"] += 1\n",
    "                report[\"table_shapes\"].append({\"page\": page_num, \"index_on_page\": i, \"shape\": df.shape})\n",
    "                print(f\"Found table on {pdf_path.name} page {page_num} (table #{i}) â€” shape {df.shape}\")\n",
    "\n",
    "    return tables, report\n",
    "\n",
    "\n",
    "def save_tables_and_summary(pdf_path: Path, tables, report, out_root: Path):\n",
    "    \"\"\"Save extracted tables as CSV and write a markdown summary for the PDF.\"\"\"\n",
    "    pdf_stem = pdf_path.stem\n",
    "    pdf_folder = out_root / pdf_stem\n",
    "    tables_folder = pdf_folder / \"tables\"\n",
    "    pdf_folder.mkdir(parents=True, exist_ok=True)\n",
    "    tables_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save each table\n",
    "    for idx, df in enumerate(tables, start=1):\n",
    "        csv_path = tables_folder / f\"table_{idx}.csv\"\n",
    "        # Try to coerce numeric columns when possible\n",
    "        try:\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        except Exception as e:\n",
    "            # fallback: save as JSON if CSV fails\n",
    "            json_path = tables_folder / f\"table_{idx}.json\"\n",
    "            df.to_json(json_path, orient=\"records\", force_ascii=False)\n",
    "            print(f\"Warning: failed to save CSV for {csv_path.name} ({e}), saved JSON instead: {json_path}\")\n",
    "\n",
    "    # Write a small markdown summary with shapes and preview rows\n",
    "    summary_md = tables_folder / \"../summary.md\"\n",
    "    with summary_md.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# Summary for {pdf_path.name}\\n\\n\")\n",
    "        f.write(f\"**Total tables found:** {report['tables_found']}\\n\\n\")\n",
    "\n",
    "        if report[\"tables_found\"] == 0:\n",
    "            f.write(\"No tables were detected in this PDF using pdfplumber's table extraction.\\n\")\n",
    "            f.write(\"Consider alternative extraction strategies (OCR, PDFMiner, manual review).\\n\")\n",
    "            return\n",
    "\n",
    "        for idx, info in enumerate(report[\"table_shapes\"], start=1):\n",
    "            f.write(f\"## Table {idx}\\n\")\n",
    "            f.write(f\"- Page: {info['page']}\\n\")\n",
    "            f.write(f\"- Shape: {info['shape']}\\n\\n\")\n",
    "            # Write a small preview of the CSV head\n",
    "            df_preview = tables[idx - 1].head(5)\n",
    "            f.write(\"```csv\\n\")\n",
    "            f.write(df_preview.to_csv(index=False))\n",
    "            f.write(\"```\\n\\n\")\n",
    "\n",
    "    print(f\"Saved {len(tables)} tables and summary for {pdf_path.name} -> {pdf_folder}\")\n",
    "\n",
    "\n",
    "def process_pdfs(pdf_paths, out_root: Path):\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    master_report = []\n",
    "\n",
    "    for pdf in pdf_paths:\n",
    "        pdf_path = Path(pdf)\n",
    "        if not pdf_path.exists():\n",
    "            print(f\"Warning: file not found: {pdf}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {pdf_path}...\")\n",
    "        tables, report = extract_tables_from_pdf(pdf_path)\n",
    "        save_tables_and_summary(pdf_path, tables, report, out_root)\n",
    "        master_report.append(report)\n",
    "\n",
    "    # Save a master JSON report listing processed PDFs\n",
    "    master_path = out_root / \"master_report.json\"\n",
    "    with master_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(master_report, f, indent=2)\n",
    "\n",
    "    print(f\"Done. Master report saved to {master_path}\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Extract tables from multiple PDF files and save CSVs + summaries.\")\n",
    "    parser.add_argument(\"pdfs\", nargs=\"+\", help=\"PDF file paths or glob patterns\")\n",
    "    parser.add_argument(\"--out\", default=\"documents\", help=\"Output root folder (default: documents)\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def expand_globs(paths):\n",
    "    expanded = []\n",
    "    for p in paths:\n",
    "        p_obj = Path(p)\n",
    "        if any(c in p for c in \"*?[]\"):\n",
    "            matched = list(p_obj.parent.glob(p_obj.name))\n",
    "            expanded.extend([str(m) for m in matched])\n",
    "        else:\n",
    "            expanded.append(str(p))\n",
    "    return expanded\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    pdf_list = expand_globs(args.pdfs)\n",
    "    out_root = Path(args.out)\n",
    "    process_pdfs(pdf_list, out_root)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
